{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-16T23:28:27.163953Z",
     "iopub.status.busy": "2025-03-16T23:28:27.163641Z",
     "iopub.status.idle": "2025-03-16T23:29:05.576507Z",
     "shell.execute_reply": "2025-03-16T23:29:05.575359Z",
     "shell.execute_reply.started": "2025-03-16T23:28:27.163930Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T23:29:05.578247Z",
     "iopub.status.busy": "2025-03-16T23:29:05.577954Z",
     "iopub.status.idle": "2025-03-16T23:29:08.634960Z",
     "shell.execute_reply": "2025-03-16T23:29:08.633976Z",
     "shell.execute_reply.started": "2025-03-16T23:29:05.578221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-16T23:29:12.064026Z",
     "iopub.status.busy": "2025-03-16T23:29:12.063639Z",
     "iopub.status.idle": "2025-03-16T23:29:17.581280Z",
     "shell.execute_reply": "2025-03-16T23:29:17.580191Z",
     "shell.execute_reply.started": "2025-03-16T23:29:12.063995Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install faiss-cpu langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-16T23:29:17.582892Z",
     "iopub.status.busy": "2025-03-16T23:29:17.582630Z",
     "iopub.status.idle": "2025-03-16T23:29:28.831888Z",
     "shell.execute_reply": "2025-03-16T23:29:28.830939Z",
     "shell.execute_reply.started": "2025-03-16T23:29:17.582869Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-16T23:29:41.121222Z",
     "iopub.status.busy": "2025-03-16T23:29:41.120889Z",
     "iopub.status.idle": "2025-03-16T23:29:42.172621Z",
     "shell.execute_reply": "2025-03-16T23:29:42.171939Z",
     "shell.execute_reply.started": "2025-03-16T23:29:41.121195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import re\n",
    "import time\n",
    "from transformers import Gemma3ForConditionalGeneration, AutoProcessor, AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved embedding function using E5 models (better for multilingual retrieval)\n",
    "class E5Embedder:\n",
    "    def __init__(self, model_name=\"intfloat/multilingual-e5-large\", device=None):\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        print(f\"Loading embedding model {model_name} on {self.device}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "        print(\"Embedding model loaded\")\n",
    "    \n",
    "    def _average_pool(self, last_hidden_states, attention_mask):\n",
    "        # Take attention mask into account for averaging\n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "    \n",
    "    def encode(self, texts, batch_size=8, show_progress_bar=True):\n",
    "        # Prepare storage for embeddings\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Process in batches to avoid OOM\n",
    "        for i in tqdm(range(0, len(texts), batch_size), disable=not show_progress_bar):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # For E5 models, add prefix for better retrieval performance\n",
    "            processed_texts = [f\"passage: {text}\" for text in batch_texts]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                processed_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                embeddings = self._average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "                \n",
    "                # Normalize embeddings\n",
    "                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "                all_embeddings.append(embeddings.cpu().numpy())\n",
    "        \n",
    "        # Concatenate all embeddings\n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def encode_queries(self, queries, batch_size=8):\n",
    "        # Similar to encode but with \"query: \" prefix instead of \"passage: \"\n",
    "        if isinstance(queries, str):\n",
    "            queries = [queries]\n",
    "            \n",
    "        processed_queries = [f\"query: {query}\" for query in queries]\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            processed_queries,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = self._average_pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced chunking function for better retrieval\n",
    "def chunk_documents(legal_data, max_chunk_size=256, overlap=50):\n",
    "    chunks = []\n",
    "    references = []\n",
    "    \n",
    "    for item in legal_data:\n",
    "        text = item[\"Текст\"]\n",
    "        reference = item[\"Ссылка\"]\n",
    "        \n",
    "        # For very short texts, keep them as is\n",
    "        if len(text.split()) <= max_chunk_size:\n",
    "            chunks.append(text)\n",
    "            references.append(reference)\n",
    "            continue\n",
    "        \n",
    "        # Split longer texts into chunks with overlap\n",
    "        words = text.split()\n",
    "        current_position = 0\n",
    "        \n",
    "        while current_position < len(words):\n",
    "            end_position = min(current_position + max_chunk_size, len(words))\n",
    "            chunk = \" \".join(words[current_position:end_position])\n",
    "            \n",
    "            # Add extra information about the source to each chunk\n",
    "            # This helps maintain context even in chunks\n",
    "            ref_info = f\"{reference} - Фрагмент {current_position//max_chunk_size + 1}\"\n",
    "            \n",
    "            chunks.append(chunk)\n",
    "            references.append(ref_info)\n",
    "            \n",
    "            # Move with overlap\n",
    "            current_position += max_chunk_size - overlap\n",
    "    \n",
    "    return chunks, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create improved FAISS index\n",
    "def create_improved_faiss_index(legal_data, embedder, chunk_size=256, overlap=50):\n",
    "    print(\"Chunking documents...\")\n",
    "    chunks, references = chunk_documents(legal_data, max_chunk_size=chunk_size, overlap=overlap)\n",
    "    print(f\"Created {len(chunks)} chunks from {len(legal_data)} documents\")\n",
    "    \n",
    "    # Create embeddings\n",
    "    print(\"Creating embeddings...\")\n",
    "    embeddings = embedder.encode(chunks)\n",
    "    \n",
    "    # Create FAISS index\n",
    "    print(\"Building FAISS index...\")\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    # Using Flat index for better accuracy\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index, chunks, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved document search with better scoring\n",
    "def search_documents(query, index, chunks, references, embedder, top_k=8, min_score=0.2):\n",
    "    # Create query embedding\n",
    "    query_embedding = embedder.encode_queries(query)\n",
    "    \n",
    "    # Search the index\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Collect results\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx < len(chunks) and scores[0][i] > min_score:\n",
    "            results.append({\n",
    "                \"text\": chunks[idx],\n",
    "                \"reference\": references[idx],\n",
    "                \"score\": float(scores[0][i])\n",
    "            })\n",
    "    \n",
    "    # If we found few or no results, try with query expansion\n",
    "    if len(results) < 3:\n",
    "        expanded_query = expand_query(query)\n",
    "        if expanded_query != query:\n",
    "            exp_embedding = embedder.encode_queries(expanded_query)\n",
    "            exp_scores, exp_indices = index.search(exp_embedding, top_k)\n",
    "            \n",
    "            # Add expanded results\n",
    "            for i, idx in enumerate(exp_indices[0]):\n",
    "                if idx < len(chunks) and exp_scores[0][i] > min_score:\n",
    "                    # Check if this document is already in results\n",
    "                    is_duplicate = False\n",
    "                    for existing in results:\n",
    "                        if existing[\"reference\"] == references[idx]:\n",
    "                            is_duplicate = True\n",
    "                            break\n",
    "                    \n",
    "                    if not is_duplicate:\n",
    "                        results.append({\n",
    "                            \"text\": chunks[idx],\n",
    "                            \"reference\": references[idx],\n",
    "                            \"score\": float(exp_scores[0][i])\n",
    "                        })\n",
    "    \n",
    "    # Sort by score\n",
    "    results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    \n",
    "    # Deduplicate by reference while keeping highest scores\n",
    "    seen_references = set()\n",
    "    deduplicated_results = []\n",
    "    \n",
    "    for result in results:\n",
    "        # Extract the base reference by removing the \"- Фрагмент X\" part\n",
    "        base_ref = result[\"reference\"].split(\" - Фрагмент\")[0]\n",
    "        \n",
    "        if base_ref not in seen_references:\n",
    "            seen_references.add(base_ref)\n",
    "            deduplicated_results.append(result)\n",
    "    \n",
    "    return deduplicated_results[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query expansion to improve retrieval\n",
    "def expand_query(query):\n",
    "    # Simple keyword extraction and expansion\n",
    "    legal_keywords = {\n",
    "        \"налог\": [\"налогообложение\", \"налоговый вычет\", \"налоговая ставка\"],\n",
    "        \"квартир\": [\"недвижимость\", \"жилье\", \"собственность\", \"жилая площадь\"],\n",
    "        \"земл\": [\"земельный участок\", \"кадастр\", \"земельный налог\"],\n",
    "        \"наследств\": [\"наследование\", \"наследодатель\", \"наследник\"],\n",
    "        \"договор\": [\"сделка\", \"соглашение\", \"контракт\"],\n",
    "        \"дар\": [\"дарение\", \"дарственная\", \"даритель\", \"одаряемый\"],\n",
    "        \"пошлин\": [\"государственная пошлина\", \"сбор\", \"платеж\"],\n",
    "        \"регистрац\": [\"регистрация\", \"росреестр\", \"оформление\"],\n",
    "        \"опек\": [\"опекунство\", \"попечительство\", \"несовершеннолетний\"]\n",
    "    }\n",
    "    \n",
    "    lower_query = query.lower()\n",
    "    expanded = query\n",
    "    \n",
    "    for keyword, expansions in legal_keywords.items():\n",
    "        if keyword in lower_query:\n",
    "            # Add one random expansion from the list\n",
    "            import random\n",
    "            expanded += f\" {random.choice(expansions)}\"\n",
    "    \n",
    "    return expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved legal question detection\n",
    "def is_legal_question(query):\n",
    "    # Extended list of legal keywords and patterns\n",
    "    legal_keywords = [\n",
    "        'закон', 'право', 'суд', 'иск', 'договор', 'налог', 'кодекс', 'льгот', \n",
    "        'штраф', 'юрист', 'адвокат', 'нотариус', 'наследств', 'имуществ',\n",
    "        'обязательств', 'ответственност', 'регистрац', 'доверенност', 'лиценз',\n",
    "        'патент', 'собственност', 'аренд', 'залог', 'ипотек', 'кредит', \n",
    "        'страхов', 'компенсац', 'возмещени', 'претензи', 'банкротств', \n",
    "        'увольнени', 'оформить', 'заявлени', 'документ', 'выплат', 'пени', \n",
    "        'кадастр', 'недвижимост', 'земл', 'квартир', 'дом', 'участ', 'гражданск',\n",
    "        'паспорт', 'снилс', 'инн', 'удостоверени', 'срок', 'пошлин', 'выписк',\n",
    "        'свидетельств', 'доход', 'пенси', 'социальн', 'пособи', 'льгот',\n",
    "        'дарени', 'продаж', 'покупк', 'наследов', 'завещани', 'брак', 'развод',\n",
    "        'алимент', 'опек', 'попечит', 'усыновле', 'гражданств', 'вид на жительство',\n",
    "        'миграци', 'прописк', 'регистрац', 'юридическ', 'физическ', 'лиц'\n",
    "    ]\n",
    "    \n",
    "    # Convert query to lowercase\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # Check for legal keywords\n",
    "    for keyword in legal_keywords:\n",
    "        if keyword in query_lower:\n",
    "            return True\n",
    "    \n",
    "    # Check for question formulations about rights and obligations\n",
    "    legal_patterns = [\n",
    "        r'как\\s+.*\\s+(оформить|получить|подать|зарегистрировать)',\n",
    "        r'что\\s+.*\\s+(делать|нужно|требуется)\\s+.*\\s+(если|для|при)',\n",
    "        r'когда\\s+.*\\s+(необходимо|нужно|следует|можно|обязан)',\n",
    "        r'где\\s+.*\\s+(оформ|получ|зарегистр|подать)',\n",
    "        r'сколько\\s+.*\\s+(стоит|платить|налог|штраф|пошлина|срок)',\n",
    "        r'какие\\s+.*\\s+(документы|права|обязанности)',\n",
    "        r'кто\\s+.*\\s+(имеет право|должен|обязан)',\n",
    "        r'можно ли\\s+.*',\n",
    "        r'обязан ли\\s+.*',\n",
    "        r'нужно ли\\s+.*',\n",
    "        r'(дарение|налог|договор|пошлина|срок)\\s+.*\\?'\n",
    "    ]\n",
    "    \n",
    "    for pattern in legal_patterns:\n",
    "        if re.search(pattern, query_lower):\n",
    "            return True\n",
    "    \n",
    "    # Be more lenient - assume it's legal if it ends with a question mark\n",
    "    if query_lower.strip().endswith('?'):\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved answer generation\n",
    "def generate_answer(query, context_documents, model, processor, max_new_tokens=384, temperature=0.7, is_legal=True):\n",
    "    if not is_legal:\n",
    "        return \"Извините, я могу отвечать только на юридические вопросы. Пожалуйста, задайте вопрос о законодательстве, правах, налогах, документах или иных юридических аспектах.\"\n",
    "    \n",
    "    # Format prompt based on context\n",
    "    if not context_documents or len(context_documents) == 0:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": \"Ты профессиональный юрист, который отвечает на вопросы о российском законодательстве. Давай точные и информативные ответы, основанные на актуальном законодательстве РФ. Если ты не уверен, честно скажи об этом и дай общую информацию по теме. Твои ответы должны быть краткими, структурированными и понятными для обычного человека.\"}]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": f\"Вопрос: {query}\"}]\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        # Prepare context, sorted by relevance\n",
    "        context_docs = sorted(context_documents, key=lambda x: x[\"score\"], reverse=True)\n",
    "        \n",
    "        # Limit the number of documents to fit within context window\n",
    "        max_docs = min(5, len(context_docs))\n",
    "        context = \"\"\n",
    "        for i, doc in enumerate(context_docs[:max_docs]):\n",
    "            context += f\"Документ {i+1}:\\n{doc['text']}\\nИсточник: {doc['reference']}\\n\\n\"\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": \"Ты профессиональный юрист, который отвечает на вопросы о российском законодательстве. Используй предоставленные фрагменты законодательства для составления ответа. Не упоминай в ответе фразы типа 'на основании предоставленной информации' или 'в предоставленных документах'. Просто дай юридически точный ответ, ссылаясь на соответствующие нормы закона. Если информации недостаточно, дай полезный ответ на основе общих знаний о законодательстве РФ, но отметь, что это общая информация. Твои ответы должны быть структурированными, краткими и понятными для обычного человека.\"}]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": f\"Вопрос: {query}\\n\\nФрагменты законодательства:\\n{context}\"}]\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    # Optimize memory - free CUDA cache before generation\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Tokenize messages with processor\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages, \n",
    "        add_generation_prompt=True, \n",
    "        tokenize=True,\n",
    "        return_dict=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=torch.bfloat16)\n",
    "    \n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    \n",
    "    # Measure response time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Generate response with parameters for speed\n",
    "        with torch.inference_mode():\n",
    "            generation = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1,\n",
    "                num_beams=1\n",
    "            )\n",
    "            \n",
    "            # Get only new tokens (excluding input)\n",
    "            generation = generation[0][input_len:]\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            # In case of memory shortage, try with lower settings\n",
    "            torch.cuda.empty_cache()\n",
    "            # Reduce input context to save memory\n",
    "            if context_documents:\n",
    "                context = \"\"\n",
    "                for i, doc in enumerate(context_docs[:2]):  # Take only 2 documents\n",
    "                    context += f\"Документ {i+1}:\\n{doc['text'][:300]}...\\nИсточник: {doc['reference']}\\n\\n\"\n",
    "                \n",
    "                messages[1][\"content\"][0][\"text\"] = f\"Вопрос: {query}\\n\\nФрагменты законодательства (сокращенные):\\n{context}\"\n",
    "                \n",
    "                inputs = processor.apply_chat_template(\n",
    "                    messages, \n",
    "                    add_generation_prompt=True, \n",
    "                    tokenize=True,\n",
    "                    return_dict=True, \n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(model.device, dtype=torch.bfloat16)\n",
    "                \n",
    "                input_len = inputs[\"input_ids\"].shape[-1]\n",
    "                \n",
    "                with torch.inference_mode():\n",
    "                    generation = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=256,\n",
    "                        temperature=0.7,\n",
    "                        do_sample=True,\n",
    "                        num_beams=1\n",
    "                    )\n",
    "                    \n",
    "                    generation = generation[0][input_len:]\n",
    "            else:\n",
    "                return \"Извините, произошла ошибка при генерации ответа. Попробуйте упростить вопрос или задать его позже.\"\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # Decode response\n",
    "    response = processor.decode(generation, skip_special_tokens=True)\n",
    "    \n",
    "    # Measure generation time\n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"Generation time: {generation_time:.2f} seconds\")\n",
    "    \n",
    "    # Post-process response for better readability\n",
    "    response = response.strip()\n",
    "    \n",
    "    # Replace some template phrases for better user experience\n",
    "    response = response.replace(\"На основании предоставленной информации\", \"Согласно законодательству РФ\")\n",
    "    response = response.replace(\"в предоставленных документах\", \"в российском законодательстве\")\n",
    "    response = response.replace(\"документах найти невозможно\", \"нормативных актах следует отметить\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format final answer with sources\n",
    "def format_answer_with_sources(answer, context_documents):\n",
    "    if not context_documents:\n",
    "        return answer\n",
    "    \n",
    "    full_response = answer + \"\\n\\nИсточники:\\n\"\n",
    "    \n",
    "    # Add only unique sources\n",
    "    unique_sources = {}\n",
    "    for doc in context_documents:\n",
    "        # Extract the base reference without the \"- Фрагмент X\" part\n",
    "        ref = doc['reference'].split(\" - Фрагмент\")[0]\n",
    "        score = doc['score']\n",
    "        if ref not in unique_sources or score > unique_sources[ref]['score']:\n",
    "            unique_sources[ref] = {'score': score}\n",
    "    \n",
    "    # Sort sources by relevance\n",
    "    sorted_sources = sorted(unique_sources.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "    \n",
    "    for i, (ref, info) in enumerate(sorted_sources):\n",
    "        full_response += f\"{i+1}. {ref} (релевантность: {info['score']:.2f})\\n\"\n",
    "    \n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chatbot with better error handling and memory optimization\n",
    "def run_improved_legal_chatbot(model_name=\"google/gemma-3-4b-it\", embedding_model_name=\"intfloat/multilingual-e5-large\", legal_data_path=None, quantization=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # CUDA memory management\n",
    "    if torch.cuda.is_available():\n",
    "        # Try to free unused memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Check available memory\n",
    "        try:\n",
    "            total_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # in GB\n",
    "            allocated_mem = torch.cuda.memory_allocated(0) / (1024**3)  # in GB\n",
    "            free_mem = total_mem - allocated_mem\n",
    "            print(f\"GPU memory: total {total_mem:.2f} GB, free {free_mem:.2f} GB\")\n",
    "        except RuntimeError:\n",
    "            print(\"Unable to check GPU memory\")\n",
    "            free_mem = 0\n",
    "    else:\n",
    "        free_mem = 0\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        print(\"Loading legal data...\")\n",
    "        if not legal_data_path:\n",
    "            print(\"ERROR: No path to legal data file specified\")\n",
    "            return None, None, None, None, None, None\n",
    "        legal_data = load_data(legal_data_path)\n",
    "        print(f\"Loaded {len(legal_data)} legal documents\")\n",
    "        \n",
    "        # Choose appropriate embedding model based on available resources\n",
    "        if free_mem < 4.0 or device.type == \"cpu\":\n",
    "            # For limited resources, use a smaller model\n",
    "            if embedding_model_name == \"intfloat/multilingual-e5-large\":\n",
    "                embedding_model_name = \"intfloat/multilingual-e5-small\"\n",
    "                print(f\"Switching to smaller embedding model due to memory constraints: {embedding_model_name}\")\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        embedder = E5Embedder(model_name=embedding_model_name, device=device)\n",
    "        \n",
    "        # Create improved FAISS index with optimal chunking\n",
    "        index, chunks, references = create_improved_faiss_index(legal_data, embedder)\n",
    "        \n",
    "        # Memory management - move embedder to CPU if memory is limited\n",
    "        if torch.cuda.is_available() and free_mem < 4.0:\n",
    "            embedder.model = embedder.model.cpu()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"Embedding model moved to CPU to save CUDA memory\")\n",
    "        \n",
    "        # Load LLM model with optimizations\n",
    "        print(\"\\nLoading language model...\")\n",
    "        if quantization:\n",
    "            # Try 8-bit quantization to save memory\n",
    "            try:\n",
    "                model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "                    model_name, \n",
    "                    device_map=\"auto\",\n",
    "                    load_in_8bit=True\n",
    "                ).eval()\n",
    "                print(\"Model loaded with 8-bit quantization\")\n",
    "            except:\n",
    "                # If that fails, try standard variant\n",
    "                print(\"Failed to load model with 8-bit quantization, using standard variant\")\n",
    "                model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "                    model_name, \n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=torch.bfloat16\n",
    "                ).eval()\n",
    "        else:\n",
    "            model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "                model_name, \n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.bfloat16\n",
    "            ).eval()\n",
    "        \n",
    "        processor = AutoProcessor.from_pretrained(model_name)\n",
    "        \n",
    "        return model, processor, embedder, index, chunks, references\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during initialization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-16T23:53:45.371Z",
     "iopub.execute_input": "2025-03-16T23:30:17.886290Z",
     "iopub.status.busy": "2025-03-16T23:30:17.885661Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Main function to start chatbot\n",
    "def start_improved_chatbot(model, processor, embedder, index, chunks, references, interactive=True, test_queries=None):\n",
    "    if test_queries:\n",
    "        # Process test queries\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TEST QUERIES\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"Question: {query}\")\n",
    "            \n",
    "            # Check if question is legal\n",
    "            legal_question = is_legal_question(query)\n",
    "            if not legal_question:\n",
    "                print(\"Chatbot response:\")\n",
    "                print(\"Извините, я могу отвечать только на юридические вопросы. Пожалуйста, задайте вопрос о законодательстве, правах, налогах, документах или иных юридических аспектах.\")\n",
    "                continue\n",
    "            \n",
    "            # Search for relevant documents\n",
    "            context_documents = search_documents(query, index, chunks, references, embedder, top_k=5, min_score=0.2)\n",
    "            \n",
    "            # Output found documents for debugging\n",
    "            print(f\"Found relevant documents: {len(context_documents)}\")\n",
    "            for i, doc in enumerate(context_documents[:3]):  # Show only first 3 for brevity\n",
    "                print(f\"Document {i+1} (relevance: {doc['score']:.4f}):\")\n",
    "                print(f\"Reference: {doc['reference']}\")\n",
    "                print(f\"Fragment: {doc['text'][:100]}...\")\n",
    "            \n",
    "            # Generate answer\n",
    "            answer = generate_answer(query, context_documents, model, processor, is_legal=legal_question)\n",
    "            \n",
    "            # Format full response with sources\n",
    "            full_response = format_answer_with_sources(answer, context_documents)\n",
    "            \n",
    "            print(\"\\nChatbot response:\")\n",
    "            print(full_response)\n",
    "    \n",
    "    if interactive:\n",
    "        # Interactive mode\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Interactive mode (type 'exit' to end):\")\n",
    "        \n",
    "        while True:\n",
    "            user_query = input(\"\\nYour question: \")\n",
    "            \n",
    "            if user_query.lower() in ['выход', 'exit', 'quit']:\n",
    "                break\n",
    "            \n",
    "            # Check if question is legal\n",
    "            legal_question = is_legal_question(user_query)\n",
    "            \n",
    "            if not legal_question:\n",
    "                print(\"Chatbot response:\")\n",
    "                print(\"Извините, я могу отвечать только на юридические вопросы. Пожалуйста, задайте вопрос о законодательстве, правах, налогах, документах или иных юридических аспектах.\")\n",
    "                continue\n",
    "            \n",
    "            # Search for relevant documents\n",
    "            context_documents = search_documents(user_query, index, chunks, references, embedder, top_k=5, min_score=0.2)\n",
    "            \n",
    "            # Generate answer\n",
    "            answer = generate_answer(user_query, context_documents, model, processor, is_legal=legal_question)\n",
    "            \n",
    "            # Format full response with sources\n",
    "            full_response = format_answer_with_sources(answer, context_documents)\n",
    "            \n",
    "            print(\"Chatbot response:\")\n",
    "            print(full_response)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    legal_data_path = \"/kaggle/input/legal-house-json/legal_documents.json\"  # Path to your JSON file\n",
    "    \n",
    "    # Initialize models and index\n",
    "    model, processor, embedder, index, chunks, references = run_improved_legal_chatbot(\n",
    "        model_name=\"google/gemma-3-4b-it\",\n",
    "        embedding_model_name=\"intfloat/multilingual-e5-small\",  # More memory efficient model\n",
    "        legal_data_path=legal_data_path,\n",
    "        quantization=True\n",
    "    )\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"Failed to initialize chatbot. Check errors above.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        \"Сколько в год налог на землю?\",\n",
    "        \"Как можно подарить квартиру?\",\n",
    "        \"Для чего нужен кадастровый номер?\",\n",
    "        \"Как рассчитать налог на квартиру?\",\n",
    "        \"Что обязательно должно быть в договоре купли продажи?\"\n",
    "    ]\n",
    "    \n",
    "    # Start chatbot with test queries\n",
    "    start_improved_chatbot(model, processor, embedder, index, chunks, references, \n",
    "                  interactive=True, test_queries=test_queries)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6709186,
     "sourceId": 10999638,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
