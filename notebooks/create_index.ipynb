{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание индексированной базы знаний для юридического чат-бота\n",
    "\n",
    "Этот ноутбук создает FAISS индекс для юридической базы данных и сохраняет его для использования в других проектах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Класс для создания эмбеддингов текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Улучшенная функция эмбеддинга с использованием моделей E5 (лучше для многоязычного поиска)\n",
    "class E5Embedder:\n",
    "    def __init__(self, model_name=\"intfloat/multilingual-e5-small\", device=None):\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        print(f\"Loading embedding model {model_name} on {self.device}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "        print(\"Embedding model loaded\")\n",
    "    \n",
    "    def _average_pool(self, last_hidden_states, attention_mask):\n",
    "        # Take attention mask into account for averaging\n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "    \n",
    "    def encode(self, texts, batch_size=8, show_progress_bar=True):\n",
    "        # Prepare storage for embeddings\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Process in batches to avoid OOM\n",
    "        for i in tqdm(range(0, len(texts), batch_size), disable=not show_progress_bar):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # For E5 models, add prefix for better retrieval performance\n",
    "            processed_texts = [f\"passage: {text}\" for text in batch_texts]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                processed_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                embeddings = self._average_pool(outputs.last_hidden_state, inputs[\"attention_mask\"])\n",
    "                \n",
    "                # Normalize embeddings\n",
    "                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "                all_embeddings.append(embeddings.cpu().numpy())\n",
    "        \n",
    "        # Concatenate all embeddings\n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def encode_queries(self, queries, batch_size=8):\n",
    "        # Similar to encode but with \"query: \" prefix instead of \"passage: \"\n",
    "        if isinstance(queries, str):\n",
    "            queries = [queries]\n",
    "            \n",
    "        processed_queries = [f\"query: {query}\" for query in queries]\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            processed_queries,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = self._average_pool(outputs.last_hidden_state, inputs[\"attention_mask\"])\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции для обработки и индексирования документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "def load_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Улучшенная функция разбиения документов на чанки для лучшего поиска\n",
    "def chunk_documents(legal_data, max_chunk_size=256, overlap=50):\n",
    "    chunks = []\n",
    "    references = []\n",
    "    \n",
    "    for item in legal_data:\n",
    "        text = item[\"Текст\"]\n",
    "        reference = item[\"Ссылка\"]\n",
    "        \n",
    "        # Для очень коротких текстов, оставляем как есть\n",
    "        if len(text.split()) <= max_chunk_size:\n",
    "            chunks.append(text)\n",
    "            references.append(reference)\n",
    "            continue\n",
    "        \n",
    "        # Разбиваем более длинные тексты на чанки с перекрытием\n",
    "        words = text.split()\n",
    "        current_position = 0\n",
    "        \n",
    "        while current_position < len(words):\n",
    "            end_position = min(current_position + max_chunk_size, len(words))\n",
    "            chunk = \" \".join(words[current_position:end_position])\n",
    "            \n",
    "            # Добавляем дополнительную информацию об источнике к каждому чанку\n",
    "            # Это помогает сохранить контекст даже в чанках\n",
    "            ref_info = f\"{reference} - Фрагмент {current_position//max_chunk_size + 1}\"\n",
    "            \n",
    "            chunks.append(chunk)\n",
    "            references.append(ref_info)\n",
    "            \n",
    "            # Перемещаемся с перекрытием\n",
    "            current_position += max_chunk_size - overlap\n",
    "    \n",
    "    return chunks, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание улучшенного FAISS индекса\n",
    "def create_improved_faiss_index(legal_data, embedder, chunk_size=256, overlap=50):\n",
    "    print(\"Chunking documents...\")\n",
    "    chunks, references = chunk_documents(legal_data, max_chunk_size=chunk_size, overlap=overlap)\n",
    "    print(f\"Created {len(chunks)} chunks from {len(legal_data)} documents\")\n",
    "    \n",
    "    # Создание эмбеддингов\n",
    "    print(\"Creating embeddings...\")\n",
    "    embeddings = embedder.encode(chunks)\n",
    "    \n",
    "    # Создание FAISS индекса\n",
    "    print(\"Building FAISS index...\")\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    # Используем Flat индекс для лучшей точности\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index, chunks, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для сохранения индекса и компонентов\n",
    "def save_index_components(index, chunks, references, embedder_model_name, output_path):\n",
    "    # Создаем директорию, если ее не существует\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    # Сохраняем FAISS индекс\n",
    "    index_path = os.path.join(output_path, \"legal_index.faiss\")\n",
    "    faiss.write_index(index, index_path)\n",
    "    \n",
    "    # Сохраняем чанки и references\n",
    "    data_path = os.path.join(output_path, \"chunks_references.pkl\")\n",
    "    with open(data_path, \"wb\") as f:\n",
    "        pickle.dump({\"chunks\": chunks, \"references\": references, \"embedder_model\": embedder_model_name}, f)\n",
    "    \n",
    "    print(f\"Индекс и компоненты успешно сохранены в {output_path}\")\n",
    "    print(f\"Путь к индексу: {index_path}\")\n",
    "    print(f\"Путь к данным: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание и сохранение индекса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполнение индексации в Kaggle\n",
    "legal_data_path = \"C:/Users/ten-t/Desktop/LegalGuardian/data/json/legal_documents.json\"  # Путь к JSON файлу в Kaggle\n",
    "output_dir = \"C:/Users/ten-t/Desktop/LegalGuardian/data/index\"  # Место для сохранения индекса в Kaggle\n",
    "embedding_model_name = \"intfloat/multilingual-e5-small\"  # Используем small модель\n",
    "\n",
    "# Инициализируем embedder\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используем устройство: {device}\")\n",
    "embedder = E5Embedder(model_name=embedding_model_name, device=device)\n",
    "\n",
    "# Загружаем данные\n",
    "print(f\"Загружаем юридические данные из {legal_data_path}...\")\n",
    "legal_data = load_data(legal_data_path)\n",
    "print(f\"Загружено {len(legal_data)} юридических документов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем индекс\n",
    "index, chunks, references = create_improved_faiss_index(legal_data, embedder)\n",
    "\n",
    "# Сохраняем компоненты\n",
    "save_index_components(index, chunks, references, embedding_model_name, output_dir)\n",
    "\n",
    "print(\"Процесс создания и сохранения индекса завершен.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка сохраненного индекса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем, что файлы созданы\n",
    "index_file = os.path.join(output_dir, \"legal_index.faiss\")\n",
    "data_file = os.path.join(output_dir, \"chunks_references.pkl\")\n",
    "\n",
    "print(f\"Проверка файла индекса: {os.path.exists(index_file)}\")\n",
    "print(f\"Проверка файла данных: {os.path.exists(data_file)}\")\n",
    "\n",
    "# Выводим размер созданных файлов\n",
    "if os.path.exists(index_file) and os.path.exists(data_file):\n",
    "    index_size = os.path.getsize(index_file) / (1024 * 1024)  # размер в МБ\n",
    "    data_size = os.path.getsize(data_file) / (1024 * 1024)  # размер в МБ\n",
    "    \n",
    "    print(f\"Размер файла индекса: {index_size:.2f} МБ\")\n",
    "    print(f\"Размер файла данных: {data_size:.2f} МБ\")\n",
    "    print(f\"Общий размер: {index_size + data_size:.2f} МБ\")\n",
    "    \n",
    "    # Быстрая проверка данных\n",
    "    with open(data_file, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "        print(f\"\\nКоличество чанков: {len(data[\"chunks\"])}\")\n",
    "        print(f\"Модель эмбеддингов: {data[\"embedder_model\"]}\")\n",
    "        \n",
    "    print(\"\\nИндекс успешно создан и готов к использованию в других проектах.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
